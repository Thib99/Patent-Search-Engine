{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EAtbz-dnQf6w"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and extracting dataset"
      ],
      "metadata": {
        "id": "MTVyCvSqJQlK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvn1I6FgIxh2",
        "outputId": "270a9b09-0806-4656-df0c-c42c695794ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1va7plI-h6FDSL8-HaAi2WPWvc1WpnKKD\n",
            "From (redirected): https://drive.google.com/uc?id=1va7plI-h6FDSL8-HaAi2WPWvc1WpnKKD&confirm=t&uuid=bad80d2a-1d0f-4240-90cf-2a09c9cc6478\n",
            "To: /content/datasets.tar.gz\n",
            "100% 311M/311M [00:06<00:00, 46.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "#download the dataset\n",
        "!gdown 'https://drive.google.com/uc?id=1va7plI-h6FDSL8-HaAi2WPWvc1WpnKKD'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the file  (may take around 10 mins)\n",
        "! tar -xvzf ./datasets.tar.gz\n",
        "\n",
        "# remove the zip file\n",
        "! rm datasets.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq-m9VCkI85-",
        "outputId": "0c11d6fe-c699-4298-e88d-223535a12e53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content_JSONs/Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json\n",
            "Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TEST.json\n",
            "Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TRAIN.json\n",
            "Citation_JSONs/Citation_Train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvg-L68smX3i",
        "outputId": "a188ac06-8d15-414e-a6b9-7bccf3385bc6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/drive/MyDrive/Fac/Master/IR/output ./output"
      ],
      "metadata": {
        "id": "4gq2ihW32S-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ead9be3-a826-4d40-f9da-1ba8cd939a07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ln: failed to create symbolic link './output/output': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librairy install"
      ],
      "metadata": {
        "id": "NYxUfGXrJn6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "! pip install rank-bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug7qmaWkJnwX",
        "outputId": "5628a23e-34cc-4539-dd3f-40ff11b2592f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank-bm25) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import librairy"
      ],
      "metadata": {
        "id": "VZ-wHmP6Jsiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "import string\n",
        "\n"
      ],
      "metadata": {
        "id": "dn4kOhekLLQO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the neccessary nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')  # For lemmatization\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpZvgViIQaZd",
        "outputId": "8f86bdd1-31d7-4cc7-ddde-b07607045e28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "EAtbz-dnQf6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_data(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        contents = json.load(file)\n",
        "    return contents\n",
        "\n",
        "\n",
        "def create_tfidf_matrix(citing_dataset, nonciting_dataset, vectorizer=TfidfVectorizer()):\n",
        "    \"\"\"\n",
        "    Creates TF-IDF matrix for the given citing and non-citing datasets based on the specified text column.\n",
        "\n",
        "    Parameters:\n",
        "    citing_dataset (json)): DataFrame containing citing patents.\n",
        "    nonciting_dataset (json): DataFrame containing non-citing patents.\n",
        "    vectorizer (TfidfVectorizer, optional): TfidfVectorizer object for vectorizing text data.\n",
        "                                             Defaults to TfidfVectorizer().\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing TF-IDF matrices for citing and non-citing patents respectively.\n",
        "           (tfidf_matrix_citing, tfidf_matrix_nonciting)\n",
        "    \"\"\"\n",
        "    all_text = [patent['text'] for patent in citing_dataset + nonciting_dataset]\n",
        "\n",
        "    # Vectorizing descriptions\n",
        "    print(\"Vectorizing descriptions...\")\n",
        "    tfidf_matrix = vectorizer.fit_transform(tqdm(all_text, desc=\"TF-IDF\"))\n",
        "\n",
        "    # Since we're interested in similarities between citing and cited patents,\n",
        "    # we need to split the TF-IDF matrix back into two parts\n",
        "    split_index = len(citing_dataset)\n",
        "    tfidf_matrix_citing = tfidf_matrix[:split_index]\n",
        "    tfidf_matrix_nonciting = tfidf_matrix[split_index:]\n",
        "\n",
        "    # Size of vocabulary\n",
        "    print(\"Size of vocabulary:\", len(vectorizer.vocabulary_))\n",
        "\n",
        "    return tfidf_matrix_citing, tfidf_matrix_nonciting\n",
        "\n",
        "\n",
        "\n",
        "def get_mapping_dict(mapping_df):\n",
        "    \"\"\"\n",
        "    Creates dictionary of citing ids to non-citing id based on given dataframe (which is based on providedjson)\n",
        "\n",
        "    Parameters:\n",
        "    mapping_df (DataFrame): DataFrame containing mapping between citing and cited patents\n",
        "    Returns:\n",
        "    dict: dictionary of unique citing patent ids to list of cited patent ids\n",
        "    \"\"\"\n",
        "    mapping_dict = {}\n",
        "\n",
        "    for _, row in mapping_df.iterrows():\n",
        "        key = row[0]  # Value from column 0\n",
        "        value = row[2]  # Value from column 2\n",
        "        if key in mapping_dict:\n",
        "            mapping_dict[key].append(value)\n",
        "        else:\n",
        "            mapping_dict[key] = [value]\n",
        "\n",
        "    return mapping_dict\n",
        "\n",
        "def create_corpus(corpus, text_type):\n",
        "    \"\"\"\n",
        "    Extracts text data from a corpus based on the specified text type.\n",
        "\n",
        "    Parameters:\n",
        "    corpus (list): List of dictionaries representing patent documents.\n",
        "    text_type (str): Type of text to extract ('title', 'abstract', 'claim1', 'claims', 'description', 'fulltext').\n",
        "\n",
        "    Returns:\n",
        "    list: List of dictionaries with 'id' and 'text' keys representing each document in the corpus.\n",
        "    \"\"\"\n",
        "\n",
        "    app_ids = [doc['Application_Number'] + doc['Application_Category'] for doc in corpus]\n",
        "\n",
        "    cnt = 0 # count the number of documents without text\n",
        "    texts = []  # list of texts\n",
        "    ids_to_remove = []  # list of ids of documents without text, to remove them from the corpus\n",
        "\n",
        "    if text_type == 'title':\n",
        "        for doc in corpus:\n",
        "            try:\n",
        "                texts.append(doc['Content']['title'])\n",
        "            except: # if the document does not have a title\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "        print(f\"Number of documents without title: {cnt}\")\n",
        "\n",
        "    elif text_type == 'abstract':\n",
        "        for doc in corpus:\n",
        "            try:\n",
        "                texts.append(doc['Content']['pa01'])\n",
        "            except: # if the document does not have an abstract\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "        print(f\"Number of documents without abstract: {cnt}\")\n",
        "\n",
        "    elif text_type == 'claim1':\n",
        "        for doc in corpus:\n",
        "            try:\n",
        "                texts.append(doc['Content']['c-en-0001'])\n",
        "            except: # if the document does not have claim 1\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "        print(f\"Number of documents without claim 1: {cnt}\")\n",
        "\n",
        "    elif text_type == 'claims':\n",
        "        # all the values with the key starting with 'c-en-', each element in the final list is a list of claims\n",
        "        for doc in corpus:\n",
        "            doc_claims = []\n",
        "            for key in doc['Content'].keys():\n",
        "                if key.startswith('c-en-'):\n",
        "                    doc_claims.append(doc['Content'][key])\n",
        "            if len(doc_claims) == 0:    # if the document does not have any claims\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "            else:\n",
        "                doc_text_string = ' '.join(doc_claims)\n",
        "                texts.append(doc_text_string)\n",
        "        print(f\"Number of documents without claims: {cnt}\")\n",
        "\n",
        "    elif text_type == 'description':\n",
        "        # all the values with the key starting with 'p'\n",
        "        for doc in corpus:\n",
        "            doc_text = []\n",
        "            for key in doc['Content'].keys():\n",
        "                if key.startswith('p'):\n",
        "                    doc_text.append(doc['Content'][key])\n",
        "            if len(doc_text) == 0:  # if the document does not have any description\n",
        "                ids_to_remove.append(doc['Application_Number']+doc['Application_Category'])\n",
        "                cnt += 1\n",
        "            else:\n",
        "                doc_text_string = ' '.join(doc_text)\n",
        "                texts.append(doc_text_string)\n",
        "        print(f\"Number of documents without description: {cnt}\")\n",
        "\n",
        "    elif text_type == 'fulltext':\n",
        "        for doc in corpus:\n",
        "            doc_text = list(doc['Content'].values())\n",
        "            doc_text_string = ' '.join(doc_text)\n",
        "            texts.append(doc_text_string)\n",
        "        if cnt > 0:\n",
        "            print(f\"Number of documents without any text: {cnt}\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid text type\")\n",
        "\n",
        "    if len(ids_to_remove) > 0:\n",
        "        print(f\"Removing {len(ids_to_remove)} documents without required text\")\n",
        "        for id_ in ids_to_remove[::-1]:\n",
        "            idx = app_ids.index(id_)\n",
        "            del app_ids[idx]\n",
        "\n",
        "    # Create a list of dictionaries with app_ids and texts\n",
        "    corpus_data = [{'id': app_id, 'text': text} for app_id, text in zip(app_ids, texts)]\n",
        "\n",
        "    return corpus_data\n",
        "\n",
        "\n",
        "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
        "    \"\"\"\n",
        "    Get the true and predicted labels for the metrics calculation.\n",
        "\n",
        "    Parameters:\n",
        "    citing_to_cited_dict : dict of str : list of str\n",
        "        Mapping between citing patents and the list of their cited patents\n",
        "    recommendations_dict : dict of str : list of str\n",
        "        Mapping between citing patents and the sorted list of recommended patents\n",
        "\n",
        "    Returns:\n",
        "    list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "    int\n",
        "        Number of patents not in the citation mapping\n",
        "    \"\"\"\n",
        "    # Initialize lists to store true labels and predicted labels\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    not_in_citation_mapping = 0\n",
        "\n",
        "    # Iterate over the items in both dictionaries\n",
        "    for citing_id in recommendations_dict.keys():\n",
        "        # Check if the citing_id is present in both dictionaries\n",
        "        if citing_id in citing_to_cited_dict:\n",
        "            # If yes, append the recommended items from both dictionaries to the respective lists\n",
        "            true_labels.append(citing_to_cited_dict[citing_id])\n",
        "            predicted_labels.append(recommendations_dict[citing_id])\n",
        "        else:\n",
        "            not_in_citation_mapping += 1\n",
        "\n",
        "    return true_labels, predicted_labels, not_in_citation_mapping\n",
        "\n",
        "\n",
        "\n",
        "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
        "    \"\"\"\n",
        "    Calculate the mean Recall@k for a list of recommendations.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels : list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    predicted_labels : list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "    k : int\n",
        "        Number of recommendations to consider.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Mean Recall@k value.\n",
        "    \"\"\"\n",
        "    recalls_at_k = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate Recall@k for each recommendation list\n",
        "        true_set = set(true)\n",
        "        k = min(k, len(pred))\n",
        "        relevant_count = sum(1 for item in pred[:k] if item in true_set)\n",
        "        recalls_at_k.append(relevant_count / len(true_set))\n",
        "\n",
        "    # Calculate the mean Recall@k\n",
        "    mean_recall = sum(recalls_at_k) / len(recalls_at_k)\n",
        "\n",
        "    return mean_recall\n",
        "\n",
        "def mean_inv_ranking(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculate the mean of lists of the mean inverse rank of true relevant items\n",
        "    in the lists of sorted recommended items.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels : list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    predicted_labels : list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Mean of lists of the mean inverse rank of true relevant items.\n",
        "    \"\"\"\n",
        "    mean_ranks = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate the inverse rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        ranks = []\n",
        "        for item in true:\n",
        "            try:\n",
        "                rank = 1 / (pred.index(item) + 1)\n",
        "            except ValueError:\n",
        "                rank = 0  # If item not found, assign 0\n",
        "            ranks.append(rank)\n",
        "\n",
        "        # Calculate the mean inverse rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        mean_rank = sum(ranks) / len(ranks)\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    # Calculate the mean of the mean inverse ranks across all recommendation lists\n",
        "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks)\n",
        "\n",
        "    return mean_of_mean_ranks\n",
        "\n",
        "\n",
        "def mean_ranking(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculate the mean of lists of the mean rank of true relevant items\n",
        "    in the lists of sorted recommended items.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels : list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    predicted_labels : list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Mean of lists of the mean rank of true relevant items.\n",
        "    \"\"\"\n",
        "    mean_ranks = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate the rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        ranks = []\n",
        "        for item in true:\n",
        "            try:\n",
        "                rank = pred.index(item) + 1\n",
        "            except ValueError:\n",
        "                rank = len(pred)  # If item not found, assign the length of the list\n",
        "            ranks.append(rank)\n",
        "\n",
        "        # Calculate the mean rank of true relevant items\n",
        "        # in the recommendation list\n",
        "        mean_rank = sum(ranks) / len(ranks)\n",
        "        mean_ranks.append(mean_rank)\n",
        "\n",
        "    # Calculate the mean of the mean ranks across all recommendation lists\n",
        "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks)\n",
        "\n",
        "    return mean_of_mean_ranks\n",
        "\n",
        "\n",
        "\n",
        "def mean_average_precision(true_labels, predicted_labels, k=10):\n",
        "    \"\"\"\n",
        "    Calculate the mean Average Precision for a list of recommendations.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels : list of list\n",
        "        True relevant items for each recommendation list.\n",
        "    predicted_labels : list of list\n",
        "        Predicted recommended items for each recommendation list.\n",
        "    k : int\n",
        "        Number of recommendations to consider.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Mean Average Precision value.\n",
        "    \"\"\"\n",
        "    average_precisions = []\n",
        "\n",
        "    for true, pred in zip(true_labels, predicted_labels):\n",
        "        # Calculate Average Precision for each recommendation list\n",
        "        true_set = set(true)\n",
        "        precision_at_k = []\n",
        "        relevant_count = 0\n",
        "        for i, item in enumerate(pred[:k]):\n",
        "            if item in true_set:\n",
        "                relevant_count += 1\n",
        "                precision_at_k.append(relevant_count / (i + 1))\n",
        "        average_precision = sum(precision_at_k) / len(true_set)\n",
        "        average_precisions.append(average_precision)\n",
        "\n",
        "    # Calculate the mean Average Precision\n",
        "    mean_average_precision = sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "    return mean_average_precision\n",
        "\n",
        "def top_k_ranks(citing, cited, cosine_similarities, k=10):\n",
        "    # Create a dictionary to store the top k ranks for each citing patent\n",
        "    top_k_ranks = {}\n",
        "    for i, content_id in enumerate(citing):\n",
        "        top_k_ranks[content_id['id']] = [cited[j]['id'] for j in np.argsort(cosine_similarities[i])[::-1][:k]]\n",
        "    return top_k_ranks\n"
      ],
      "metadata": {
        "id": "czWQgpWDQiS6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset"
      ],
      "metadata": {
        "id": "hF-X4_kDQpR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_citing_train = load_json_data(\"./Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TRAIN.json\")\n",
        "json_citing_test = load_json_data(\"./Content_JSONs/Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TEST.json\")\n",
        "\n",
        "json_nonciting = load_json_data(\"./Content_JSONs/Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json\")\n",
        "json_citing_to_cited = load_json_data(\"./Citation_JSONs/Citation_Train.json\") # Citing ids are unique\n",
        "\n",
        "citing_dataset_df = pd.DataFrame(json_citing_train)\n",
        "\n",
        "nonciting_dataset_df = pd.DataFrame(json_nonciting)\n",
        "mapping_dataset_df = pd.DataFrame(json_citing_to_cited)"
      ],
      "metadata": {
        "id": "Ayqtxp6bQo8q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Dataset of the query : **citing_dataset_df**\n",
        "\n",
        "* Dataset of the DB : **nonciting_dataset_df**"
      ],
      "metadata": {
        "id": "UjFk-aC8Raoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print((nonciting_dataset_df['Application_Number'] == '2221486').unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7v08ac3lMnq",
        "outputId": "031adc81-d06e-4588-dcd7-41e0e8d8df0c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False  True]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# citing_dataset_df.iloc[0].Content"
      ],
      "metadata": {
        "id": "APlk1kPxR8IS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reall  work"
      ],
      "metadata": {
        "id": "d64P1bhpl44u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merge all text into one column\n",
        "citing_dataset_merge_df = pd.DataFrame(citing_dataset_df['Application_Number'])\n",
        "citing_dataset_merge_df['Application_Category'] = citing_dataset_df['Application_Category']\n",
        "citing_dataset_merge_df['text'] = citing_dataset_df['Content'].apply(lambda x: ' '.join(x.values()))"
      ],
      "metadata": {
        "id": "mE3PnHVqSZSy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "# Combine the stop words and punctuation into a single set\n",
        "combined_null_words = stop_words.union(punctuation)\n",
        "\n",
        "def tokenizing(text):\n",
        "    \"\"\"\n",
        "    Cette fonction prend un texte en entrée, le tokenise, supprime les mots commun,\n",
        "    met tout en minuscules et retire la ponctuation.\n",
        "\n",
        "    Args:\n",
        "    text (str): Le texte à tokeniser.\n",
        "\n",
        "    Returns:\n",
        "    list: Une liste de tokens nettoyés.\n",
        "    \"\"\"\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.lower() not in combined_null_words]\n",
        "    return tokens\n",
        "\n",
        "out = tokenizing(citing_dataset_merge_df['text'][0])\n",
        "print(out)"
      ],
      "metadata": {
        "id": "d5UBHWmt3rU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d77a6f-f28e-4eed-da56-163c4e2eef98"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['device', 'controlling', 'braking', 'trailer', 'device', '1', 'controlling', 'braking', 'trailer', 'comprises', 'one', 'control', 'line', '2', 'connectable', 'source', 'work', 'fluid', 'first', 'pressure', 'braking', 'line', '3', 'connectable', 'service', 'braking', 'system', '4', 'trailer', 'communicating', 'control', 'line', '2', 'one', 'additional', 'line', '5', 'connectable', 'source', 'work', 'fluid', 'second', 'pressure', 'one', 'emergency', 'line', '6', 'connectable', 'additional', 'line', '5', 'connectable', 'emergency', 'and/or', 'parking', 'brake', '7', 'trailer', 'type', 'hydraulically', 'released', 'spring', 'brake', 'one', 'discharge', 'line', '8', 'work', 'fluid', 'communicating', 'collection', 'tank', '9', 'first', 'valve', 'means', 'operable', 'braking', 'position', 'wherein', 'additional', 'line', '5', 'isolated', 'discharge', 'line', '8', 'emergency', 'position', 'wherein', 'additional', 'line', '5', 'communicating', 'discharge', 'line', '8', 'second', 'valve', 'means', '17', 'positioned', 'additional', 'line', '5', 'emergency', 'line', '6', 'operable', 'braking', 'position', 'wherein', 'emergency', 'line', '6', 'communicating', 'additional', 'line', '5', 'movement', 'position', 'wherein', 'emergency', 'line', '6', 'isolated', 'additional', 'line', '5', 'device', '1', 'controlling', 'braking', 'trailer', 'comprising', 'least', 'one', 'control', 'line', '2', 'connectable', 'source', 'work', 'fluid', 'first', 'pressure', 'braking', 'line', '3', 'connectable', 'service', 'braking', 'system', '4', 'trailer', 'communicating', 'said', 'control', 'line', '2', 'least', 'one', 'additional', 'line', '5', 'connectable', 'source', 'work', 'fluid', 'second', 'pressure', 'least', 'one', 'emergency', 'line', '6', 'connectable', 'said', 'additional', 'line', '5', 'connectable', 'emergency', 'and/or', 'parking', 'brake', '7', 'trailer', 'type', 'hydraulically', 'released', 'spring', 'brake', 'least', 'one', 'discharge', 'line', '8', 'work', 'fluid', 'communicating', 'collection', 'tank', '9', 'first', 'valve', 'means', 'operable', 'braking', 'position', 'wherein', 'said', 'additional', 'line', '5', 'isolated', 'said', 'discharge', 'line', '8', 'emergency', 'position', 'wherein', 'said', 'additional', 'line', '5', 'communicating', 'said', 'discharge', 'line', '8', 'device', '1', 'according', 'claim', '1', 'characterized', 'fact', 'comprises', 'pumping', 'means', '18', 'connected', 'said', 'collection', 'tank', '9', 'operable', 'send', 'work', 'fluid', 'collection', 'tank', 'along', 'said', 'emergency', 'line', '6', 'said', 'second', 'valve', 'means', '17', 'movement', 'position', 'device', '1', 'according', 'claim', '2', 'characterized', 'fact', 'said', 'pumping', 'means', '18', 'manual', 'type', 'device', '1', 'according', 'claim', '2', '3', 'characterized', 'fact', 'said', 'tank', '9', 'said', 'emergency', 'line', '6', 'positioned', 'one-way', 'third', 'fourth', 'valve', 'means', '22', '29', 'adapted', 'prevent', 'flow', 'work', 'fluid', 'said', 'emergency', 'line', '6', 'towards', 'said', 'pumping', 'means', '18', '--', 'said', 'pumping', 'means', '18', 'towards', 'said', 'collection', 'tank', '9', 'respectively', 'device', '1', 'according', 'one', 'claims', '2', '4', 'characterized', 'fact', 'comprises', 'least', 'one', 'basic', 'body', '19', 'supporting', 'said', 'second', 'valve', 'means', '17', 'said', 'pumping', 'means', '18', 'device', '1', 'according', 'claim', '5', 'characterized', 'fact', 'said', 'collection', 'tank', '9', 'associated', 'said', 'basic', 'body', '19', 'device', '1', 'according', 'claim', '5', '6', 'characterized', 'fact', 'said', 'second', 'valve', 'means', '17', 'comprise', 'least', 'one', 'seat', '23', 'defined', 'said', 'basic', 'body', '19', 'communicating', 'said', 'additional', 'line', '5', 'said', 'emergency', 'line', '6', 'fact', 'comprises', 'least', 'one', 'spool', '24', 'housed', 'sliding', 'manner', 'inside', 'said', 'seat', '23', 'said', 'spool', '24', 'adapted', 'place', 'said', 'additional', 'line', '5', 'communication', 'said', 'emergency', 'line', '6', 'said', 'braking', 'position', 'isolate', 'said', 'additional', 'line', '5', 'said', 'emergency', 'line', '6', 'said', 'movement', 'position', 'device', '1', 'according', 'claim', '7', 'characterized', 'fact', 'said', 'spool', '24', 'least', 'first', 'section', '24a', 'first', 'thrust', 'area', 'a1', 'communicating', 'said', 'additional', 'line', '5', 'said', 'movement', 'position', 'least', 'second', 'section', '24b', 'second', 'thrust', 'area', 'a2', 'communicating', 'said', 'additional', 'line', '5', 'said', 'emergency', 'line', '6', 'said', 'movement', 'position', 'said', 'first', 'thrust', 'area', 'a1', 'larger', 'diameter', 'said', 'second', 'thrust', 'area', 'a2', 'said', 'spool', '24', 'moving', 'towards', 'said', 'braking', 'position', 'due', 'presence', 'work', 'fluid', 'pressure', 'along', 'said', 'additional', 'line', '5', 'device', '1', 'according', 'claim', '7', '8', 'characterized', 'fact', 'comprises', 'least', 'first', 'female', 'joint', '14', 'communicating', 'said', 'control', 'line', '2', 'least', 'second', 'female', 'joint', '15', 'communicating', 'said', 'additional', 'line', '5', 'said', 'discharge', 'line', '8', 'comprising', 'said', 'first', 'valve', 'means', 'said', 'first', 'second', 'female', 'joints', '14', '15', 'connectable', 'first', 'second', 'male', 'joint', 'respectively', 'associated', 'towing', 'vehicle', 'characterized', 'fact', 'comprises', 'mechanical', 'connecting', 'means', '26', 'said', 'second', 'female', 'joint', '15', 'said', 'spool', '24', 'said', 'mechanical', 'connecting', 'means', '26', 'adapted', 'move', 'said', 'spool', '24', 'towards', 'said', 'braking', 'position', 'following', 'detachment', 'said', 'female', 'joints', '14', '15', '--', 'said', 'male', 'joints', 'device', '1', 'according', 'one', 'preceding', 'claims', 'characterized', 'fact', 'comprises', 'signaling', 'means', '27', 'signaling', 'position', 'said', 'second', 'valve', 'means', '17', 'device', '1', 'according', 'claim', '10', 'characterized', 'fact', 'said', 'signaling', 'means', '27', 'comprise', 'least', 'one', 'signaling', 'element', '28', 'associated', 'movable', 'manner', 'said', 'basic', 'body', '19', 'adapted', 'operate', 'conjunction', 'said', 'spool', '24', 'said', 'signaling', 'element', '28', 'movable', 'first', 'work', 'position', 'wherein', 'engaged', 'said', 'spool', '24', 'latter', 'braking', 'position', 'second', 'work', 'position', 'wherein', 'disengaged', 'said', 'spool', '24', 'latter', 'movement', 'position', 'device', '1', 'according', 'claim', '11', 'characterized', 'fact', 'said', 'signaling', 'element', '28', 'hinged', 'said', 'basic', 'body', '19', 'fact', 'following', 'displacement', 'said', 'spool', '24', 'braking', 'position', 'movement', 'position', 'moves', 'first', 'second', 'work', 'position', 'due', 'weight', 'present', 'invention', 'relates', 'device', 'controlling', 'braking', 'trailer', 'well', 'known', 'case', 'trailer', 'towed', 'tractor', 'braking', 'systems', 'thereof', 'operationally', 'connected', 'way', 'braking', 'tractor', 'actuated', 'operator', 'also', 'causes', 'braking', 'towed', 'trailer', 'braking', 'system', 'trailer', 'controlled', 'braking', 'system', 'tractor', 'order', 'synchronize', 'braking', 'forces', 'acting', 'therefore', 'operator', 'actuates', 'tractor', \"'s\", 'brake', 'pedal', 'intervene', 'tractor', \"'s\", 'wheels', 'means', 'braking', 'valve', 'called', 'brake-trailer', 'valve', 'also', 'trailer', \"'s\", 'braking', 'system', 'well', 'known', 'today', \"'s\", 'towing', 'vehicles', 'connected', 'trailer', 'means', 'connecting', 'device', 'comprising', 'pair', 'male', 'joints', 'adapted', 'inserted', 'pair', 'relative', 'female', 'joints', 'associated', 'trailer', 'particular', 'female', 'joints', 'connected', 'control', 'line', 'adapted', 'supply', 'trailer', \"'s\", 'braking', 'system', 'additional', 'line', 'respectively', 'adapted', 'deactivate', 'emergency', 'and/or', 'parking', 'brake', 'trailer', 'emergency', 'situations', 'additional', 'line', 'must', 'properly', 'connected', 'tank', 'towing', 'vehicle', 'allow', 'activation', 'emergency', 'and/or', 'parking', 'brake', 'trailer', 'therefore', 'braking', 'valve', 'means', 'placed', 'inside', 'relative', 'joint', 'adapted', 'connect', 'case', 'detachment', 'trailer', 'towing', 'vehicle', 'additional', 'line', 'discharge', 'line', 'fluid', 'pressure', 'keeps', 'emergency', 'and/or', 'parking', 'brake', 'deactivated', 'type', 'spring', 'brake', 'hydraulic', 'release', 'called', 'sahr', 'discharged', 'collection', 'tank', 'thus', 'allowing', 'spring', 'expand', 'activate', 'automatic', 'and/or', 'parking', 'braking', 'known', 'connecting', 'devices', 'however', 'drawbacks', 'may', 'necessary', 'move', 'trailer', 'even', 'disconnected', 'towing', 'vehicle', 'allow', 'trailer', 'moved', 'disconnected', 'towing', 'vehicle', 'described', 'operating', 'condition', 'emergency', 'and/or', 'parking', 'brake', 'activated', 'due', 'connection', '--', 'additional', 'line', 'collection', 'tank', 'another', 'drawback', 'known', 'type', 'devices', 'allow', 'towing', 'trailer', 'old', 'generation', 'towing', 'vehicle', 'i.e', 'provided', 'control', 'line', 'connected', 'service', 'braking', 'system', 'main', 'aim', 'present', 'invention', 'devise', 'device', 'controlling', 'braking', 'trailer', 'allows', 'trailer', 'moved', 'even', 'disconnected', 'towing', 'vehicle', 'within', 'aim', 'one', 'object', 'present', 'invention', 'devise', 'device', 'allows', 'preventing', 'emergency', 'and/or', 'parking', 'brake', 'trailer', 'incorrectly', 'deactivated', 'even', 'connection', 'towing', 'vehicle', 'another', 'object', 'present', 'invention', 'effectively', 'signal', 'user', 'status', 'automatic', 'and/or', 'parking', 'braking', 'trailer', 'yet', 'another', 'object', 'devise', 'device', 'controlling', 'braking', 'trailer', 'allows', 'trailer', 'towed', 'also', 'means', 'old', 'generation', 'towing', 'vehicles', 'i.e', 'vehicles', 'single-line', 'type', 'another', 'object', 'present', 'invention', 'devise', 'device', 'controlling', 'braking', 'trailer', 'allows', 'overcoming', 'aforementioned', 'drawbacks', 'prior', 'art', 'context', 'simple', 'rational', 'easy', 'effective', 'use', 'low-cost', 'solution', 'aforementioned', 'objects', 'achieved', 'present', 'device', 'controlling', 'braking', 'trailer', 'according', 'claim', '1', 'characteristics', 'advantages', 'present', 'invention', 'evident', 'description', 'preferred', 'exclusive', 'embodiment', 'device', 'controlling', 'braking', 'trailer', 'illustrated', 'way', 'nonlimiting', 'example', 'accompanying', 'tables', 'drawing', 'figure', '1', 'schematic', 'representation', 'braking', 'system', 'trailer', 'comprising', 'device', 'according', 'invention', 'first', 'embodiment', 'figure', '2', 'schematic', 'representation', 'braking', 'system', 'trailer', 'comprising', 'device', 'according', 'invention', 'second', 'embodiment', 'figure', '3', 'side', 'elevation', 'view', 'device', 'according', 'invention', 'spool', 'braking', 'position', 'figure', '4', 'section', 'device', 'figure', '3', 'according', 'track', 'plane', 'iv-iv', '--', 'figure', '5', 'side', 'elevation', 'view', 'device', 'shown', 'figure', '3', 'spool', 'movement', 'position', 'figure', '6', 'section', 'device', 'figure', '5', 'according', 'track', 'plane', 'vi-vi', 'particular', 'reference', 'illustrations', 'reference', 'numeral', '1', 'globally', 'indicates', 'device', 'controlling', 'braking', 'trailer', 'device', '1', 'comprises', 'least', 'one', 'control', 'line', '2', 'connectable', 'source', 'work', 'fluid', 'e.g', 'oil', 'first', 'pressure', 'one', 'braking', 'line', '3', 'connectable', 'service', 'braking', 'system', '4', 'trailer', 'communicating', 'control', 'line', '2', 'least', 'one', 'additional', 'line', '5', 'connectable', 'source', 'work', 'fluid', 'second', 'pressure', 'least', 'one', 'emergency', 'line', '6', 'connectable', 'additional', 'line', '5', 'connectable', 'emergency', 'and/or', 'parking', 'brake', '7', 'trailer', 'comprises', 'least', 'one', 'discharge', 'line', '8', 'work', 'fluid', 'communicating', 'collection', 'tank', '9', 'device', '1', 'comprises', 'first', 'valve', 'means', 'visible', 'detail', 'figures', 'operable', 'braking', 'position', 'wherein', 'additional', 'line', '5', 'isolated', 'discharge', 'line', '8', 'emergency', 'position', 'wherein', 'additional', 'line', '5', 'communicating', 'discharge', 'line', '8', 'braking', 'position', 'therefore', 'work', 'fluid', 'present', 'additional', 'line', '5', 'sent', 'emergency', 'and/or', 'parking', 'brake', '7', 'means', 'emergency', 'line', '6', 'emergency', 'position', 'work', 'fluid', 'present', 'along', 'additional', 'line', '5', 'discharged', 'collection', 'tank', '9', 'term', '``', 'first', 'valve', 'means', \"''\", 'meant', 'contained', 'joint', '15', 'allow', 'line', '5', 'connected', 'line', '8', 'result', 'detachment', 'female', 'joints', 'male', 'joints', 'specifically', 'emergency', 'and/or', 'parking', 'brake', '7', 'comprises', 'actuating', 'rod', '10', 'control', 'cylinder', '11', 'piston', '11a', 'mechanically', 'connected', 'rod', 'movable', 'braking', 'position', 'release', 'position', 'inside', 'cylinder', '11', 'elastic', 'means', '12', 'housed', 'adapted', 'push', 'piston', '11a', 'towards', 'braking', 'position', 'inside', 'cylinder', '11', 'defined', 'chamber', '11b', 'counteracts', 'elastic', 'means', '12', '--', 'fed', 'fluid', 'along', 'emergency', 'line', '6', 'embodiments', 'shown', 'figures', '1', '2', 'differ', 'manner', 'emergency', 'and/or', 'parking', 'brake', '7', 'implemented', 'appropriately', 'emergency', 'and/or', 'parking', 'brakes', '7', 'also', 'serve', 'service', 'brakes', '4', 'depending', 'operating', 'condition', 'particular', 'first', 'embodiment', 'figure', '1', 'piston', '11a', 'operates', 'directly', 'rod', '10', 'result', 'thrust', 'exerted', 'elastic', 'means', '12', 'inside', 'channel', '30', 'flow', 'work', 'fluid', 'obtained', 'communicates', 'braking', 'line', '3', 'operates', 'rod', '10', 'normal', 'operating', 'conditions', 'i.e', 'female', 'joints', '14', '15', 'connected', 'relative', 'male', 'joints', 'pressurized', 'fluid', 'present', 'along', 'control', 'line', '2', 'operates', 'rod', '10', 'adjusting', 'service', 'braking', 'trailer', 'time', 'pressurized', 'fluid', 'coming', 'additional', 'line', '5', 'enters', 'chamber', '11b', 'compress', 'elastic', 'means', '12', 'follows', 'piston', '11a', 'exert', 'force', 'rod', '10', 'operating', 'condition', 'brake', 'question', 'serves', 'service', 'brake', '4', 'instant', 'female', 'joints', '14', '15', 'detach', 'male', 'joints', 'pressure', 'along', 'control', 'lines', '2', 'braking', 'lines', '3', 'reset', 'zero', 'time', 'work', 'fluid', 'contained', 'chamber', '11b', 'discharged', 'collection', 'tank', '9', 'follows', 'therefore', 'elastic', 'means', '12', 'push', 'piston', '11a', 'towards', 'rod', '10', 'causing', 'displacement', 'thereof', 'operating', 'condition', 'brake', 'question', 'serves', 'emergency', 'and/or', 'parking', 'brake', '7', 'second', 'embodiment', 'figure', '2', 'piston', '11a', 'mechanically', 'connected', 'rod', '10', 'means', 'relevant', 'cable', '13', 'detail', 'cable', '13', 'associated', 'one', 'containment', 'element', '11c', 'rod', '10', 'inside', 'channel', '30', 'communicating', 'braking', 'line', '3', 'acting', 'rod', 'also', 'case', 'rod', '10', 'moved', 'due', 'effect', 'pressure', 'along', 'braking', 'line', '3', 'thus', 'serving', 'service', 'brake', '4', 'due', 'effect', 'displacement', 'containment', 'element', '11c', 'due', 'pulling', 'force', 'exerted', 'cable', '13', 'thereon', 'result', 'displacement', 'piston', '11a', 'following', 'emptying', 'chamber', '11b', 'action', 'elastic', 'means', '12', 'containment', 'element', '11c', 'therefore', 'acts', 'alternative', '--', 'way', 'rod', '10', 'pressure', 'exerted', 'work', 'fluid', 'present', 'along', 'braking', 'line', '3', 'thus', 'avoiding', 'sum', 'effects', 'preferred', 'embodiments', 'shown', 'figures', 'device', '1', 'comprises', 'first', 'female', 'joint', '14', 'communicating', 'control', 'line', '2', 'connectable', 'first', 'male', 'joint', 'shown', 'figures', 'associated', 'towing', 'vehicle', 'second', 'female', 'joint', '15', 'communicating', 'additional', 'line', '5', 'discharge', 'line', '8', 'comprising', 'first', 'valve', 'means', 'second', 'female', 'joint', '15', 'connected', 'second', 'male', 'joint', 'shown', 'figures', 'associated', 'towing', 'vehicle', 'according', 'invention', 'device', '1', 'comprises', 'least', 'second', 'valve', 'means', '17', 'positioned', 'additional', 'line', '5', 'emergency', 'line', '6', 'operable', 'braking', 'position', 'wherein', 'connect', 'emergency', 'line', '6', 'additional', 'line', '5', 'movement', 'position', 'wherein', 'isolate', 'emergency', 'line', '6', 'additional', 'line', '5', 'advantageously', 'device', '1', 'also', 'comprises', 'pumping', 'means', '18', 'connected', 'collection', 'tank', '9', 'operable', 'send', 'work', 'fluid', 'collection', 'tank', 'along', 'emergency', 'line', '6', 'second', 'valve', 'means', '17', 'movement', 'position', 'pumping', 'means', '18', 'preferably', 'manual', 'type', 'detail', 'device', '1', 'comprises', 'least', 'one', 'basic', 'body', '19', 'supporting', 'second', 'valve', 'means', '17', 'pumping', 'means', '18', 'shown', 'figures', 'collection', 'tank', '9', 'directly', 'associated', 'basic', 'body', '19', 'preferably', 'pumping', 'means', '18', 'emergency', 'line', '6', 'positioned', 'one-way', 'third', 'valve', 'means', '22', 'adapted', 'prevent', 'return', 'flow', 'work', 'fluid', 'emergency', 'line', '6', 'collection', 'tank', '9', 'addition', 'pumping', 'means', '18', 'collection', 'tank', '9', 'positioned', 'fourth', 'valve', 'means', '29', 'adapted', 'prevent', 'return', 'flow', 'fluid', 'collection', 'tank', 'specifically', 'pumping', 'means', '18', 'comprise', 'least', 'one', 'lever', '18a', 'operated', 'manually', 'operator', 'adapted', 'move', 'small', 'cylinder', '18b', 'inserted', 'smoothly', 'tightly', 'seat', '18c', 'obtained', 'basic', '--', 'body', '19', 'communicating', 'taking', 'channel', '20', 'along', 'fourth', 'valve', 'means', '29', 'arranged', 'collection', 'tank', '9', 'seat', '18c', 'communicating', 'sending', 'channel', '21', 'emergency', 'line', '6', 'along', 'sending', 'channel', '21', 'third', 'valve', 'means', '22', 'arranged', 'advantageously', 'second', 'valve', 'means', '17', 'comprise', 'least', 'one', 'seat', '23', 'defined', 'basic', 'body', '19', 'communicating', 'additional', 'line', '5', 'emergency', 'line', '6', 'least', 'one', 'defined', 'section', 'inside', 'basic', 'body', 'comprise', 'least', 'one', 'spool', '24', 'housed', 'sliding', 'manner', 'inside', 'seat', '23', 'spool', '24', 'adapted', 'place', 'additional', 'line', '5', 'communication', 'emergency', 'line', '6', 'braking', 'position', 'pressurized', 'fluid', 'supplied', 'towing', 'vehicle', 'chamber', '11b', 'second', 'female', 'joint', '15', 'discharged', 'chamber', '11b', 'collection', 'tank', '9', 'isolate', 'additional', 'line', '5', 'emergency', 'line', '6', 'movement', 'position', 'work', 'fluid', 'taken', 'collection', 'tank', '9', 'means', 'pumping', 'means', '18', 'sent', 'emergency', 'and/or', 'parking', 'brake', '7', 'means', 'emergency', 'line', '6', 'preferably', 'spool', '24', 'least', 'first', 'section', '24a', 'first', 'thrust', 'area', 'a1', 'second', 'section', '24b', 'second', 'thrust', 'area', 'a2', 'diameter', 'first', 'thrust', 'area', 'a1', 'identified', 'figures', 'φ1', 'larger', 'diameter', 'second', 'thrust', 'area', 'a2', 'identified', 'figures', 'φ2', 'spool', '24', 'movement', 'position', 'first', 'thrust', 'area', 'a1', 'communicating', 'additional', 'line', '5', 'second', 'thrust', 'area', 'a2', 'communicating', 'additional', 'line', '5', 'emergency', 'line', '6', 'means', 'additional', 'line', '5', 'pressure', 'due', 'connection', 'towing', 'vehicle', 'spool', '24', 'moves', 'autonomously', 'braking', 'position', 'due', 'greater', 'thrust', 'area', 'pressurized', 'fluid', 'operates', 'particular', 'spool', '24', 'movement', 'position', 'emergency', 'line', '6', 'communicating', 'diameter', 'φ2', 'therefore', 'second', 'thrust', 'area', 'a2', 'therefore', 'generate', 'forces', 'spool', '24', 'additional', 'line', '5', 'communicates', 'one', 'hand', 'first', 'diameter', 'φ1', 'therefore', 'first', 'thrust', 'area', 'a1', 'hand', 'second', 'diameter', 'φ2', 'therefore', 'second', 'thrust', 'area', 'a2', '--', 'appropriately', 'additional', 'line', '5', 'emergency', 'line', '6', 'face', 'seat', '23', 'means', 'additional', 'port', '5a', 'emergency', 'port', '6a', 'respectively', 'sealing', 'means', '25', 'positioned', 'words', 'sealing', 'means', '25', 'positioned', 'first', 'section', '24a', 'second', 'section', '24b', 'spool', '24', 'movement', 'position', 'first', 'section', '24a', 'protrudes', 'outside', 'basic', 'body', '19', 'spool', '24', 'braking', 'position', 'completely', 'contained', 'inside', 'basic', 'body', 'spool', '24', 'movement', 'position', 'advantageously', 'device', '1', 'comprises', 'mechanical', 'connecting', 'means', '26', 'second', 'female', 'joint', '15', 'spool', '24', 'adapted', 'move', 'spool', 'towards', 'braking', 'position', 'result', 'detachment', 'female', 'joints', '14', '15', 'relative', 'male', 'joints', 'particular', 'connecting', 'means', '26', 'first', 'bring', 'spool', '24', 'braking', 'position', 'subsequently', 'release', 'female', 'joints', '14', '15', 'relative', 'male', 'joints', 'connecting', 'means', '26', 'e.g.', 'type', 'inextensible', 'cable', 'preferably', 'device', '1', 'comprises', 'signaling', 'means', '27', 'signaling', 'position', 'second', 'valve', 'means', '17', 'preferred', 'embodiments', 'shown', 'figures', 'signaling', 'means', '27', 'comprise', 'least', 'one', 'signaling', 'element', '28', 'associated', 'movable', 'manner', 'basic', 'body', '19', 'adapted', 'operate', 'conjunction', 'spool', '24', 'specifically', 'signaling', 'element', '28', 'movable', 'first', 'work', 'position', 'wherein', 'engages', 'spool', '24', 'latter', 'braking', 'position', 'second', 'work', 'position', 'wherein', 'disengaged', 'spool', '24', 'latter', 'movement', 'position', 'signaling', 'element', '28', 'adapted', 'move', 'first', 'work', 'position', 'second', 'work', 'position', 'result', 'displacement', 'spool', '24', 'braking', 'position', 'movement', 'position', 'signaling', 'element', '28', 'comprises', 'fastening', 'element', '28a', 'engages', 'first', 'section', '24a', 'spool', '24', 'latter', 'braking', 'position', 'spool', '24', 'moves', 'movement', 'position', 'result', 'entry', 'first', 'section', '24a', 'basic', 'body', '19', 'fastening', 'element', '28a', 'disengages', 'first', 'section', '--', 'signaling', 'element', '28', 'preferably', 'hinged', 'basic', 'body', '19', 'result', 'displacement', 'spool', '24', 'braking', 'position', 'movement', 'position', 'moves', 'first', 'work', 'position', 'second', 'work', 'position', 'due', 'weight', 'operation', 'device', 'according', 'invention', 'follows', 'towing', 'operations', 'i.e', 'female', 'joints', '14', '15', 'connected', 'relevant', 'male', 'joints', 'associated', 'towing', 'vehicle', 'control', 'line', '2', 'sends', 'work', 'fluid', 'coming', 'braking', 'system', 'towing', 'vehicle', 'braking', 'line', '3', 'activate', 'service', 'braking', 'system', '4', 'trailer', 'additional', 'line', '5', 'sends', 'work', 'fluid', 'emergency', 'line', '6', 'keep', 'emergency', 'and/or', 'parking', 'brake', '7', 'deactivated', 'means', 'compression', 'relevant', 'elastic', 'means', '12', 'operating', 'condition', 'first', 'valve', 'means', 'braking', 'position', 'additional', 'line', '5', 'isolated', 'discharge', 'line', '8', 'therefore', 'collection', 'tank', '9', 'female', 'joints', '14', '15', 'disconnected', 'relevant', 'male', 'joints', 'additional', 'line', '5', 'placed', 'communication', 'discharge', 'line', '8', 'first', 'valve', 'means', 'move', 'braking', 'position', 'emergency', 'position', 'result', 'connection', 'additional', 'line', '5', 'discharge', 'line', '8', 'work', 'fluid', 'present', 'inside', 'chamber', '11b', 'emergency', 'and/or', 'parking', 'brake', '7', 'keeps', 'relevant', 'elastic', 'means', '12', 'compressed', 'discharged', 'inside', 'collection', 'tank', '9', 'allowing', 'emergency', 'and/or', 'parking', 'brake', '7', 'activate', 'operating', 'conditions', 'second', 'valve', 'means', '17', 'particularly', 'spool', '24', 'kept', 'braking', 'position', 'signaling', 'element', '28', 'first', 'work', 'position', 'trailer', 'needs', 'moved', 'spool', '24', 'manually', 'moved', 'movement', 'position', 'thus', 'interrupting', 'connection', 'additional', 'line', '5', 'emergency', 'line', '6', 'operating', 'configuration', 'operator', 'operate', 'pumping', 'means', '18', 'sending', 'work', 'fluid', 'contained', 'collection', 'tank', '9', 'along', 'emergency', 'line', '6', 'inside', 'emergency', 'and/or', 'parking', 'brake', '7', 'deactivate', 'compressing', '--', 'relevant', 'elastic', 'means', '12', 'result', 'movement', 'spool', '24', 'movement', 'position', 'signaling', 'element', '28', 'moves', 'second', 'work', 'position', 'due', 'weight', 'thus', 'indicating', 'emergency', 'and/or', 'parking', 'brake', '7', 'deactivated', 'event', 'result', 'connection', 'female', 'joints', '14', '15', 'relevant', 'male', 'joints', 'additional', 'line', '5', 'returns', 'second', 'work', 'position', 'even', 'operator', 'forgets', 'returning', 'spool', '24', 'braking', 'position', 'result', 'different', 'thrust', 'area', 'operate', 'work', 'fluid', 'present', 'additional', 'line', '5', 'fluid', 'present', 'along', 'emergency', 'line', '6', 'spool', '24', 'moves', 'autonomously', 'braking', 'position', 'thus', 'connecting', 'additional', 'line', '5', 'emergency', 'line', '6', 'result', 'connection', 'female', 'joints', '14', '15', 'male', 'joints', 'first', 'valve', 'means', 'automatically', 'brought', 'back', 'braking', 'position', 'additional', 'line', '5', 'isolated', 'discharge', 'line', '8', 'operator', 'manually', 'returns', 'signaling', 'element', '28', 'first', 'work', 'position', 'event', 'towing', 'vehicle', 'old', 'generation', 'type', 'i.e', 'provided', 'single', 'male', 'joint', 'adapted', 'connect', 'first', 'female', 'joint', '14', 'therefore', 'control', 'line', '2', 'connection', 'made', 'spool', '24', 'manually', 'moved', 'movement', 'position', 'work', 'fluid', 'sent', 'collection', 'tank', '9', 'emergency', 'line', '6', 'means', 'pumping', 'means', '18', 'deactivates', 'emergency', 'and/or', 'parking', 'brake', '7', 'allows', 'trailer', 'towed', 'event', 'towing', 'vehicle', 'detached', 'trailer', 'cable', '13', 'exerts', 'pulling', 'force', 'spool', '24', 'returns', 'braking', 'position', 'place', 'emergency', 'line', '6', 'communication', 'additional', 'line', '5', 'allow', 'work', 'fluid', 'kept', 'elastic', 'means', '12', 'compressed', 'flow', 'collection', 'tank', '9', 'thus', 'causing', 'activation', 'emergency', 'and/or', 'parking', 'brake', '7', 'practice', 'found', 'described', 'invention', 'achieves', 'intended', 'objects', 'particular', 'underlined', 'device', 'present', 'invention', 'relates', 'allows', 'thanks', 'presence', 'second', 'valve', 'means', '--', 'pumping', 'means', 'moving', 'trailer', 'deactivating', 'corresponding', 'emergency', 'and/or', 'parking', 'brake', 'even', 'trailer', 'detached', 'relevant', 'towing', 'vehicle', 'device', 'controlling', 'braking', 'according', 'present', 'invention', 'also', 'allows', 'avoiding', 'emergency', 'and/or', 'parking', 'brake', 'remain', 'deactivated', 'even', 'connected', 'towing', 'vehicle', 'thanks', 'fact', 'spool', 'sections', 'different', 'diameters', 'furthermore', 'device', 'present', 'invention', 'relates', 'makes', 'possible', 'promptly', 'inform', 'user', 'second', 'valve', 'means', 'braking', 'movement', 'position', 'thus', 'informing', 'operating', 'conditions', 'good', 'time', 'last', 'least', 'device', 'controlling', 'braking', 'according', 'invention', 'allows', 'correct', 'towing', 'trailer', 'even', 'single-line', 'old', 'generation', 'towing', 'vehicle']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_significant_word(text, top_k=200) :\n",
        "    bm25 = BM25Okapi([text])\n",
        "    score = []\n",
        "    df = pd.DataFrame(text, columns=['token'])\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        query = df['token'][i]\n",
        "        df.loc[i, 'score'] = bm25.get_scores([query])[0]\n",
        "\n",
        "    return df.sort_values(by=['score'], ascending=False)[:top_k]\n",
        "\n",
        "out_2 = get_most_significant_word(out, top_k=6)\n",
        "print(out_2)"
      ],
      "metadata": {
        "id": "QiFAo2K-pf6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5876918-c769-49bb-b870-fb9902c8b06c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              token     score\n",
            "2789   deactivating -0.274653\n",
            "2790  corresponding -0.274653\n",
            "2848           last -0.274653\n",
            "2814         remain -0.274653\n",
            "2846           good -0.274653\n",
            "1139          vi-vi -0.274653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def do_poss_tagging(text, pos_tags=None) :\n",
        "  if pos_tags is None :\n",
        "    nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n",
        "    verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
        "    adverbes = ['RB', 'RBR', 'RBS']\n",
        "    pos_tags = nouns + verbs + adverbes #what type to keep\n",
        "\n",
        "  out = nltk.pos_tag(text)\n",
        "\n",
        "  out = [word for word, pos in out if pos in pos_tags]\n",
        "\n",
        "  return out\n",
        "\n",
        "out_3 = do_poss_tagging(out_2['token'])\n",
        "print(out_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGvnfPjGtTS_",
        "outputId": "a026b64a-b36a-4e8f-eb01-c8ba50a188da"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['deactivating', 'corresponding', 'vi-vi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# citing_dataset_merge_df['text'] = citing_dataset_merge_df['text'].apply(lambda x: get_most_significant_word([x], top_k=200)['token'].tolist())"
      ],
      "metadata": {
        "id": "RkBzgjT0qIGa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms_list(word_list):\n",
        "  def get_synonyms(word):\n",
        "\n",
        "      # look for the synonyms, discard same word has results\n",
        "      for syn in wordnet.synsets(word):\n",
        "          for lemma in syn.lemmas():\n",
        "              if lemma.name()!= word :\n",
        "                return lemma.name()\n",
        "      return None\n",
        "\n",
        "  for word in word_list:\n",
        "    synonym = get_synonyms(word)\n",
        "    if synonym:\n",
        "      word_list.append(synonym)\n",
        "  return word_list\n",
        "\n",
        "# out_4 = get_synonyms_list(out_3[:20])\n",
        "# print(out_4)\n"
      ],
      "metadata": {
        "id": "5zNuski_xA6-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(out_4)"
      ],
      "metadata": {
        "id": "kmMc0u7qxZA0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_location = \"./output/\"\n",
        "def save_df(df, save_path):\n",
        "    df.to_json(save_path)"
      ],
      "metadata": {
        "id": "3LI-MLekm_Xl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipe_saver(df, top_k=500, save=True):\n",
        "\n",
        "  name = f\"prepro_top_{top_k}\"\n",
        "  # create the dir of the the reponse\n",
        "  if save :\n",
        "      # Create the directory\n",
        "      print(save_location+name)\n",
        "      save_location_path = save_location+name +\"/\"\n",
        "      os.makedirs(save_location_path, exist_ok=True)\n",
        "\n",
        "  # first tokenize\n",
        "  df['token'] = df['text'].apply(lambda x: tokenizing(x))\n",
        "  if save :\n",
        "    save_df(df, save_location_path + \"tokenized.json\")\n",
        "\n",
        "  # then get the most significant word\n",
        "  df.loc['token'] = df['token'].apply(lambda x: get_most_significant_word(x, top_k=top_k)['token'].tolist())\n",
        "  if save :\n",
        "    save_df(df, save_location_path + \"most_significant_word.json\")\n",
        "\n",
        "  # postaging\n",
        "  df.loc['token'] = df['token'].apply(lambda x: do_poss_tagging(x))\n",
        "  if save :\n",
        "    save_df(df, save_location_path + \"postagged.json\")\n",
        "\n",
        "  # get synonyms\n",
        "  # df['token'] = df['token'].apply(lambda x: get_synonyms_list(x))\n",
        "\n",
        "  # if save :\n",
        "  #   save_df(df, save_location_path + \"full_pipeline.json\")\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "1Do4YiW8lYUY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# citing_dataset_merge_df[1].text"
      ],
      "metadata": {
        "id": "rdbWEQjvhJ4N"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_saver(citing_dataset_merge_df[:2], top_k=100, save=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "V1n5WWc-1hiY",
        "outputId": "4ab061be-2705-400e-a86a-942f302d9261"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./output/prepro_top_100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-ccd4ef0dd92e>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['token'] = df['text'].apply(lambda x: tokenizing(x))\n",
            "<ipython-input-21-ccd4ef0dd92e>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.loc['token'] = df['token'].apply(lambda x: get_most_significant_word(x, top_k=top_k)['token'].tolist())\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'float' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9e160f3f7c2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe_saver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mciting_dataset_merge_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-ccd4ef0dd92e>\u001b[0m in \u001b[0;36mpipe_saver\u001b[0;34m(df, top_k, save)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# postaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdo_poss_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msave_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_location_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"postagged.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-ccd4ef0dd92e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;31m# postaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdo_poss_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msave_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_location_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"postagged.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-6591b216e4a6>\u001b[0m in \u001b[0;36mdo_poss_tagging\u001b[0;34m(text, pos_tags)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnouns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mverbs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madverbes\u001b[0m \u001b[0;31m#what type to keep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[1;32m    168\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             tag, conf = (\n",
            "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
          ]
        }
      ]
    }
  ]
}